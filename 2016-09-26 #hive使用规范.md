# Hive 应用规范

1. Hive DataBase 介绍
DB库名称
DB库用途
default
默认库
Dw_mb
Dw 移动库
Dw_pc
Dw PC库
pc
Pc 端统计落地表库
naga
移动端统计落地表库
test
存放临时表库

## 2. Hive表命名标准

序号 | 表名格式 | 描述
---|---|---
1 | rpt_ | 生成报表所使用的表
2 | tmp_ | 临时表
3 | dim_ | 维度表
4 | fct_ | Int或bigint

## 3. 字段后缀使用规范

序号 | 字段 | 描述
---|---|---
1 | 后缀_DATE | String，  格式yyyy-mm-dd，例如：2012-3-14
2 | 后缀_TIME | 格式yyyy-mm-dd hh-mi-ss,例如：2012-3-14 15:02:22
3 | 后缀_AMT  | Double, 存储金额类型的字段，AMT为amount缩写
4 | 后缀_NUM  | Int或bigint
5 | 后缀_PRICE| row 2 col 2
6 | 后缀_CNT  | bigint , 存储次数类型字段，例如点击次数
7 | 后缀_RATE | Double, 存储比例字段对应值，例如，佣金比例

## 4. Hive Job格式要求

贴出/home/hadoop/abcplus_workspace/pc_pjct/utm_visi/utm_visit.sh为例，
内容如下：

```shell
#!/bin/bash
# by zhuokun 2014-11-27  session_info 统计
. /etc/profile[A1]

if [ $# == 1 ]; then
   date=$1
else
   date=`date -d -1days '+%Y-%m-%d'`
fi[A2]
cd /home/hadoop/abcplus_workspace/pc_pjct/utm_visit

hiveF ./script/rpt_chnl_visits_daily.sql -date $date[A3]
if test $? -ne 0
then
exit 11
fi[A4]

dump exec ./dump/utm_visit.properties -date $date[A5]
if test $? -ne 0
then
exit 12
fi
```

## 5 对临时表有3点要求

1. 表名规范TMP_xxx_xxx_
TMP是前缀，后边根据作业业务命名，务必保证唯一，作业间临时表名相同会造成数据问题。
2. 其他作业不得引用该临时表。
3. 建议有日期字段，方便核查数据，一般不保留历史数据（非增量表）。

## 6 注意事项

1. 手工对增量表的查询，必须限制分区字段（通常是日期分区ds）且limit行数，如：

```sql
Select  url from dw.pc_track_log where date=’2013-11-12’ limit 2;
```

如果不加ds限制，则会全表扫描生成大MapReduce作业以堵塞集群；

不limit行数则会刷屏，此时可Ctrl + c 停止

```shell
Hadoop job –kill job_xxxxx
```

2. 提交作业后，需看一下http://hostname:50030/jobtracker.jsp查看map和ruduce数是否合理。此方法可以避免手误产生大Map的问题，也可核对参数传递是否正确。

## 其他

引入环境变量

参数设定默认值，需支持传参

执行统计逻辑

关键步骤必须进行返回值判断

导出到关系mysql
