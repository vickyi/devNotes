Hive UDF 更新上线
====

[TOC]

## hiveUDF项目代码变更
> 如果hiveUDF项目代码变更，就需要hive001上的hiveUDF.jar文件，测试通过后邮件架构上线

```
## 开发时，确保在dev分支
git status

git add .

git commit -m "xxxxxxxx 本次的改动"

## 改完代码后进行打包，只有编译成功后的jar才能提交。以下命令执行成功后复制到hive001对应的目录下。
mvn clean package

```

## jar包发布

> 如果代码有新的功能改动，一定要将pom文件中定义的版本号升级一下

```
## jar包发布
mvn clean deploy
```

## 更新hiveUDF用到的dim_page表文件

> 如果dim_page表有变化，则需要将hiveUDF用到的dim_page文件推送到hdfs上。如果没有变化，此步骤可以跳过。

1. hiveUDF用到的mysql测试库192.168.19.12（具体见hiveUDF项目代码）。如果是在测试库上改了mysql，还需要保持同步到db-master-biabcplus-000.abcplus下的dw.dim_page。
2. 同步dim_page表以后，找架构调度一下宙斯作业442
3. 按照如下命令，将dim_page文件发布到线上hdfs

```
## 登陆spark001, 将配置文件上传到此目录
[hadoop@ABCBI-SPARK-001 hive_udf]$ pwd
/data/users/vk/hive_udf

## 查看配置文件
hadoop fs -ls /user/hadoop/dw_realtime/config/

## 删除配置文件
hadoop fs -rm /user/hadoop/dw_realtime/config/*.properties

## 上传新的配置文件
hadoop fs -put -f PageID.properties /user/hadoop/dw_realtime/config/PageID.properties
hadoop fs -put -f PcPageValue.properties /user/hadoop/dw_realtime/config/PcPageValue.properties

```

## hive001 上jar包功能测试
```
## 将HiveUDF.jar上传到此目录
cd /data/udf_test
```

## hive001上 hive的init 文件路径
```
ll /data/abcplus_workspace/software/conf/hive/init.hql

## 在hive001上测试udf，执行以下命令，加载init文件
cd /data/abcplus_workspace/software/conf/hive/
hive -i ./init.hql
```

 ## hdfs 增删查等基本操作
```
## 查看
hadoop fs -ls /user/hadoop/dw_realtime/config/

## 删除文件
hadoop fs -rm /user/hadoop/dw_realtime/config/id.properties

## 删除目录及文件
hadoop fs -rmr /user/hadoop/dw_realtime/config/
# or
hadoop fs -rm -r /user/hadoop/dw_realtime/config/


## 创建目录
hadoop fs -mkdir /user/hadoop/dw_realtime/config/

## 上传文件
hadoop fs -put ./id.properties /user/hadoop/dw_realtime/config/

## 查看文件全部内容
hadoop fs -cat /user/hadoop/dw_realtime/config/id.properties

## 查看文件结尾1000字符
hadoop fs -tail /user/hadoop/dw_realtime/config/id.properties

## 将hdfs文件拉取到指定目录下
hadoop fs -get /user/hadoop/dw_realtime/config/id.properties ./
```