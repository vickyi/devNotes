[TOC]
## 实时消费
## 重新消费
### 方案1
```
# spark 脚本
consumerType=2

# kafkaParams
"auto.offset.reset" -> "smallest"
```

### 方案2
```
# spark 脚本
"consumerType"="2"

## 指定时间点
"consumerTime"=2017030600
```

### 数据目录
```

```
## 小文件合并
```
## hdfs-fils-merge

## ops001

scp /data/jenkins_workspace/workspace/hdfs-files-merge/hdfs-files-merge/target/hdfs-files-merge-1.0.jar hadoop@spark001.abcplus:/home/hadoop/users/vk/hdfs_smallfiles
hdfs-files-merge-1.0.jar
```

```
## 实时消费数据路径
hadoop fs -du -h hdfs://nameservice1/user/hadoop/dw_realtime/dw_real_for_path_list/mb_event_hash2/

hadoop fs -du -h hdfs://nameservice1/user/hadoop/dw_realtime/dw_real_for_path_list/mb_pageinfo_hash2/


## 重新消费数据路径
hadoop fs -du -h hdfs://nameservice1/user/hadoop/dw_realtime/reprod/mb_event_hash2/

hadoop fs -du -h hdfs://nameservice1/user/hadoop/dw_realtime/reprod/mb_pageinfo_hash2/
```


## spark 001
```
yarn jar ./hdfs-files-merge-1.0.jar com.abcplus.bi.merge.CleanHistoricalData 2017-02-25
```

```
yarn jar ./hdfs-files-merge-1.0.jar com.abcplus.bi.merge.TaskManager 2017-03-04
```

git remote add origin https://gitlab.abcplus.org/bi-source/hiveUDF.git
```
## 代码测试