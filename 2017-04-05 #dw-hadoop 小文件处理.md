## kafka
```
scp /data/jenkins_workspace/workspace/reprod_bi_realtime_by_dw_mb_pageinfo_hash2/realtime-sourcedata/target/realtime-souredata-1.0.jar hadoop@spark001.abcplus:/home/hadoop/users/vk/spark_reprod/

```

## hdfs-fils-merge
```
## ops001
scp /data/jenkins_workspace/workspace/path_list/pathlist/target/pathlist-1.0.jar hadoop@spark001.abcplus:/data/users/vk/test_path_list/

```

### 执行 Test
```
cd /home/hadoop/users/vk/test_path_list
## test 需要指定数据输出路径
## 数据输出目录：
yarn jar ./pathlist-newpath.jar com.abcplus.bi.mapred.PathListControledJobs 2017-03-27 test1
```

## 默认情况下，输出写正式目录
```
yarn jar ./pathlist-1.0.jar com.abcplus.bi.mapred.PathListControledJobs 2017-03-01
```

## 实时消费数据路径
```
hadoop fs -du -h hdfs://nameservice1/user/hadoop/dw_realtime/dw_real_for_path_list/mb_event_hash2/

hadoop fs -du -h hdfs://nameservice1/user/hadoop/dw_realtime/dw_real_for_path_list/mb_pageinfo_hash2/
```

## 重新消费数据路径
```
hadoop fs -du -h hdfs://nameservice1/user/hadoop/dw_realtime/reprod/mb_event_hash2/

hadoop fs -ls -h hdfs://nameservice1/user/hadoop/dw_realtime/reprod/mb_event_hash2/date=2017-03-13/gu_hash=0

hadoop fs -du -h hdfs://nameservice1/user/hadoop/dw_realtime/reprod/mb_pageinfo_hash2/
```


## spark 001
```
## 清理历史文件
yarn jar ./hdfs-files-merge-1.0.jar com.abcplus.bi.merge.CleanHistoricalData 2017-02-25

## 清理小文件
yarn jar ./hdfs-files-merge-1.0.jar com.abcplus.bi.merge.TaskManager 2017-03-04
```

git remote add origin https://gitlab.abcplus.org/bi-source/hiveUDF.git

## 合并reprod小文件
```
scp /data/jenkins_workspace/workspace/hdfs-files-merge/hdfs-files-merge/target/hdfs-files-merge-1.0.jar hadoop@spark001.abcplus:/home/hadoop/users/vk/hdfs_smallfiles/

12.4 G  37.2 G  hdfs://nameservice1/user/hadoop/dw_realtime/reprod/mb_event_hash2/date=2017-03-11
14.3 G  43.0 G  hdfs://nameservice1/user/hadoop/dw_realtime/reprod/mb_event_hash2/date=2017-03-12
13.6 G  40.9 G  hdfs://nameservice1/user/hadoop/dw_realtime/reprod/mb_event_hash2/date=2017-03-13
13.4 G  40.1 G  hdfs://nameservice1/user/hadoop/dw_realtime/reprod/mb_event_hash2/date=2017-03-14
13.4 G  40.3 G  hdfs://nameservice1/user/hadoop/dw_realtime/reprod/mb_event_hash2/date=2017-03-15
13.5 G  40.4 G  hdfs://nameservice1/user/hadoop/dw_realtime/reprod/mb_event_hash2/date=2017-03-16
13.2 G  39.7 G  hdfs://nameservice1/user/hadoop/dw_realtime/reprod/mb_event_hash2/date=2017-03-17
13.2 G  39.7 G  hdfs://nameservice1/user/hadoop/dw_realtime/reprod/mb_event_hash2/date=2017-03-18
14.3 G  43.0 G  hdfs://nameservice1/user/hadoop/dw_realtime/reprod/mb_event_hash2/date=2017-03-19
2.6 G   7.9 G   hdfs://nameservice1/user/hadoop/dw_realtime/reprod/mb_event_hash2/date=2017-03-20

yarn jar ./hdfs-files-merge-1.0.jar com.abcplus.bi.merge.TaskManager 2017-03-14

yarn jar ./hdfs-files-merge-1.0.jar com.abcplus.bi.merge.TaskManager 2017-05-06
yarn jar ./hdfs-files-merge-1.0.jar com.abcplus.bi.merge.TaskManager 2017-05-07


## 合并后的数据目录
```
hadoop fs -du -h /user/hadoop/dw_realtime/reprod/mb_pageinfo_hash2
hadoop fs -du -h /user/hadoop/dw_realtime/reprod/mb_event_hash2
hadoop fs -du -h /user/hadoop/dw_realtime/reprod/pc_events_hash3
hadoop fs -du -h /user/hadoop/dw_realtime/reprod/abcplus_hash3

```

## 实时数据目录
```
hadoop fs -ls /user/hadoop/dw_realtime/dw_real_for_path_list/

hadoop fs -ls /user/hadoop/dw_realtime/dw_real_for_path_list/mb_pageinfo_hash2
hadoop fs -ls /user/hadoop/dw_realtime/dw_real_for_path_list/mb_event_hash2
hadoop fs -ls /user/hadoop/dw_realtime/dw_real_for_path_list/pc_events_hash3
hadoop fs -ls /user/hadoop/dw_realtime/dw_real_for_path_list/abcplus_hash3

```

## 备份实时目录数据
```
hadoop fs -mkdir /user/hadoop/dw_realtime/bak_real/mb_pageinfo_hash2
hadoop fs -mkdir /user/hadoop/dw_realtime/bak_real/mb_event_hash2
hadoop fs -mkdir /user/hadoop/dw_realtime/bak_real/pc_events_hash3
hadoop fs -mkdir /user/hadoop/dw_realtime/bak_real/abcplus_hash3

## 备份实时目录数据 2017-05-07
hadoop fs -mv /user/hadoop/dw_realtime/dw_real_for_path_list/mb_pageinfo_hash2/date=2017-05-07 /user/hadoop/dw_realtime/bak_real/mb_pageinfo_hash2/
hadoop fs -mv /user/hadoop/dw_realtime/dw_real_for_path_list/mb_event_hash2/date=2017-05-07 /user/hadoop/dw_realtime/bak_real/mb_event_hash2/
hadoop fs -mv /user/hadoop/dw_realtime/dw_real_for_path_list/pc_events_hash3/date=2017-05-07 /user/hadoop/dw_realtime/bak_real/pc_events_hash3/
hadoop fs -mv /user/hadoop/dw_realtime/dw_real_for_path_list/abcplus_hash3/date=2017-05-07 /user/hadoop/dw_realtime/bak_real/abcplus_hash3/

## 备份实时目录数据 2017-05-06
hadoop fs -mv /user/hadoop/dw_realtime/dw_real_for_path_list/mb_pageinfo_hash2/date=2017-05-06 /user/hadoop/dw_realtime/bak_real/mb_pageinfo_hash2/
hadoop fs -mv /user/hadoop/dw_realtime/dw_real_for_path_list/mb_event_hash2/date=2017-05-06 /user/hadoop/dw_realtime/bak_real/mb_event_hash2/
hadoop fs -mv /user/hadoop/dw_realtime/dw_real_for_path_list/pc_events_hash3/date=2017-05-06 /user/hadoop/dw_realtime/bak_real/pc_events_hash3/
hadoop fs -mv /user/hadoop/dw_realtime/dw_real_for_path_list/abcplus_hash3/date=2017-05-06 /user/hadoop/dw_realtime/bak_real/abcplus_hash3/

## 备份实时目录数据 2017-05-08
hadoop fs -mv /user/hadoop/dw_realtime/dw_real_for_path_list/mb_pageinfo_hash2/date=2017-05-08 /user/hadoop/dw_realtime/bak_real/mb_pageinfo_hash2/
hadoop fs -mv /user/hadoop/dw_realtime/dw_real_for_path_list/mb_event_hash2/date=2017-05-08 /user/hadoop/dw_realtime/bak_real/mb_event_hash2/
hadoop fs -mv /user/hadoop/dw_realtime/dw_real_for_path_list/pc_events_hash3/date=2017-05-08 /user/hadoop/dw_realtime/bak_real/pc_events_hash3/
hadoop fs -mv /user/hadoop/dw_realtime/dw_real_for_path_list/abcplus_hash3/date=2017-05-08 /user/hadoop/dw_realtime/bak_real/abcplus_hash3/

```

## 用重新消费的数据更新实时目录
```
hadoop fs -ls /user/hadoop/dw_realtime/reprod/mb_pageinfo_hash2/date=2017-05-07/gu_hash=[0-9a-f]/logs/*
hadoop fs -ls /user/hadoop/dw_realtime/reprod/mb_event_hash2/date=2017-05-07/gu_hash=[0-9a-f]/logs/*
hadoop fs -ls /user/hadoop/dw_realtime/reprod/pc_events_hash3/date=2017-05-07/gu_hash=[0-9a-f]/logs/*
hadoop fs -ls /user/hadoop/dw_realtime/reprod/abcplus_hash3/date=2017-05-07/gu_hash=[0-9a-f]/logs/*

## 用重新消费的数据更新实时目录
hadoop fs -mv /user/hadoop/dw_realtime/reprod/mb_pageinfo_hash2/date=2017-05-07 /user/hadoop/dw_realtime/dw_real_for_path_list/mb_pageinfo_hash2/
hadoop fs -mv /user/hadoop/dw_realtime/reprod/mb_event_hash2/date=2017-05-07 /user/hadoop/dw_realtime/dw_real_for_path_list/mb_event_hash2/
hadoop fs -mv /user/hadoop/dw_realtime/reprod/pc_events_hash3/date=2017-05-07 /user/hadoop/dw_realtime/dw_real_for_path_list/pc_events_hash3/
hadoop fs -mv /user/hadoop/dw_realtime/reprod/abcplus_hash3/date=2017-05-07 /user/hadoop/dw_realtime/dw_real_for_path_list/abcplus_hash3/

hadoop fs -mv /user/hadoop/dw_realtime/reprod/mb_pageinfo_hash2/date=2017-05-06 /user/hadoop/dw_realtime/dw_real_for_path_list/mb_pageinfo_hash2/
hadoop fs -mv /user/hadoop/dw_realtime/reprod/mb_event_hash2/date=2017-05-06 /user/hadoop/dw_realtime/dw_real_for_path_list/mb_event_hash2/
hadoop fs -mv /user/hadoop/dw_realtime/reprod/pc_events_hash3/date=2017-05-06 /user/hadoop/dw_realtime/dw_real_for_path_list/pc_events_hash3/
hadoop fs -mv /user/hadoop/dw_realtime/reprod/abcplus_hash3/date=2017-05-06 /user/hadoop/dw_realtime/dw_real_for_path_list/abcplus_hash3/

hadoop fs -mv /user/hadoop/dw_realtime/reprod/mb_pageinfo_hash2/date=2017-05-08 /user/hadoop/dw_realtime/dw_real_for_path_list/mb_pageinfo_hash2/
hadoop fs -mv /user/hadoop/dw_realtime/reprod/mb_event_hash2/date=2017-05-08 /user/hadoop/dw_realtime/dw_real_for_path_list/mb_event_hash2/
hadoop fs -mv /user/hadoop/dw_realtime/reprod/pc_events_hash3/date=2017-05-08 /user/hadoop/dw_realtime/dw_real_for_path_list/pc_events_hash3/
hadoop fs -mv /user/hadoop/dw_realtime/reprod/abcplus_hash3/date=2017-05-08 /user/hadoop/dw_realtime/dw_real_for_path_list/abcplus_hash3/
```

## shell计算
```
nohup sh ./fct_path_list_reprod.sh 2017-05-07 > ./fct_path_list_reprod0507.log 2>&1 &
```