# 基础数据应用组-工作内容

## 前言

数据仓库：是一个面向主题的、集成的、非易失的且随时间变化的数据集合，用来支持管理人员的决策。

数据仓库的特性之一是集成，将不同的数据来源、不同形式的数据整合到一起。

公司的数据仓库分为离线数据仓库和实时数据仓库。离线数据仓库的数据计算频率以天（包含小时、周、月）为单位，大部分作业是每天凌晨处理前一天的数据。随着业务的发展，业务方对数据实时性有了更高的要求，因此公司也逐步研发实时数据链路。

公司的数据仓库的加工链路也是遵循业界的分层理念，包括操作数据层（Operation Data Store， ODS）、数据仓库层（Data Warehouse， DW）和数据集市层（Data Market，DM）。

数据连路上，与业界不同的地方是，公司的流量数据是汇聚在Hive的Base层。因此，流量数据的链路是：Base → DW → DM | RPT（Report， 报表）。

通过数据仓库不同层次之间的加工过程，实现从数据资产向信息资产的转化，并且对整个过程进行有效的数据质量处理。

### 项目组源码

基础数据组ETL项目（脚本）：https://gitlab.abcplus.org/bi-source/dw_etl.git

hiveUDF：https://gitlab.abcplus.org/bi-source/hiveUDF.git

### 公司数据研发岗的工作

基础数据应用团队的基础工作：数据采集 → 数据同步 → 数据仓库建模，输出高质量的数据，支撑各种数据产品，支撑业务数据服务，为业务决策、发展提供更有力的支撑。

ETL大致流程如下：

【了解需求】→ 【模型设计】→ 【ETL开发】→ 【测试】→ 【发布上线】→ 【日常运维】→ 【任务下线】

从数仓的发展之路来看，基础数据应用团队支撑全线业务的取数、报表、邮件等形式的数据需求，对运营、市场、商务等业务以及数据分析分别进行培训。协同产品梳理数据需求，引导业务方按照合理的方式提出数据需求，并及时给出所需数据。

### 公司整体数据流程

![公司整体数据流程](http://olx1ji9hn.bkt.clouddn.com/image/JP%E7%BD%91%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF.svg)

## 数据技术

### 数据采集

#### 数据采集、上报和消费链路

数据采集基本流程：
![埋点](http://olx1ji9hn.bkt.clouddn.com/image/JP%E5%9F%8B%E7%82%B9%E6%96%B9%E6%A1%88%E6%B5%81%E7%A8%8B.svg)

完整的数据采集上报链路：[5.埋点数据上报链路](http://wiki.abcplus.org/pages/viewpage.action?pageId=30012053)

### 浏览器端数据采集

前端js可以直接调用trackevent方法，该方法需要需要传入一下四个值：

字段  | 备注
------|------
e_c | 埋点的类型，现阶段这个字段主要存储埋点的归属是客服/会员/运营等各个部门需求提出
e_a | 埋点的名字，唯一标识一个埋点的用途。
e_n | 埋点值内容类型，主要用户对应解析的算法，例如点击加入购物车存储的是sku_id，那此处变标识为sku_id，BI收到日志会按照sku_id的解析方式去解析数据
e_v | 埋点值，BI会将此处值根据e_n去解析改字段中的数据

**JS 端埋点函数：**

```js
_paq.push(['trackEvent', e_c, e_a, e_n, e_v]);
```

以**点击在线客服**的埋点为例：

```js
_paq.push(['trackEvent',
 'kefu',
 'click_kefu_contact_serviceonline',
 'question_id',
 埋点值]);
```

**注意！**：

> 新增埋点时，一定要和BI确认埋点的名字和内容(e_c/e_a)！！！可以联系( 公子)
> ==>> 更加详细的说明，请参见：[PC/WAP埋点](http://wiki.abcplus.org/pages/viewpage.action?pageId=1507482)

#### App端数据采集

app埋点数据采集和上报

> 【[5.埋点数据上报链路](http://wiki.abcplus.org/pages/viewpage.action?pageId=30012053)】>> 【[Chant](http://wiki.abcplus.org/display/bi/Chant)】

各类埋点特定字段

> 基础数据应用组整理了App端各类埋点的必出传字段，针对不同时间，前端开发人员必须根据同数据组约定的格式埋点，详情参见：[OneData埋点管理](http://wiki.abcplus.org/pages/viewpage.action?pageId=40276783)

特定场景埋点方案

> 在点击事件上，有些比较特殊的场景及字段，埋点的数据格式上要求比较高，具体参见：[APP页面坑位埋点CID对应表+推荐位埋点方案](http://wiki.abcplus.org/pages/viewpage.action?pageId=6521036)

#### 遇到的问题和挑战

基础数据应用团队设计的埋点格式与开发对埋点的理解差异，导致产生了哪些冲突？

哪些情况导致埋点数据异常？

- 产品的埋点需求在项目上线前几个小时才想起来要提给BI
- BI在紧急情况下设计埋点，考虑不全面，遗漏设计或者需要做额外的开发工作
- 开发拿到紧急埋点，漏埋或者传错值

> ==>> 针对埋点需求，基础数据应用团队提出了埋点需求申请流程：[埋点需求接入管理](http://wiki.abcplus.org/pages/viewpage.action?pageId=40275903)

### 问题和挑战

1. 两套埋点，一套是公司自研的埋点；一套是诸葛IO格式的埋点。自研埋点有一定复杂性，如果不熟悉、不理解，容易埋错。

2. 埋点管理困难：自研的埋点没有进行生命周期管理，很多埋点不知道什么时候上线的，背景是什么，是否有价值。

参考业界的常用方案：[DTalk分享-前端数据采集与分析的那些事](http://wiki.abcplus.org/pages/viewpage.action?pageId=42736837)

## 数据同步

### 数据同步规范

公司的业务发展迅速，当新的业务出现时候，需要将数据同步到ODS层，数据分析人员才能进行分析。这样的同步需求都是数据分析师拿到分析需求后，要求BI同步相应的库表数据，没有邮件，仅在内部的沟通工具（企业微信或者钉钉）上提出，导致需求不清楚，责任方不明确。

> ==>> 在经历一段时间的混乱之后，DW组决定建立一个可控的规范，记录业务方的抽数申请，避免口口相传，首先由分析师填写 【1. 抽数申请】，然后数据开发人员跟进处理。

**实行几个月之后，数据同步这块基本流畅。**

### 批量同步

数据同步的方案经历了两个阶段。

第一阶段：

```mermaid
graph LR
A[业务mysql] -->B(BI MySQL ODS层)
B --> C[BI Hive ODS层]
```

第二阶段：

```mermaid
graph LR
A[业务mysql] -->B(BI Hive ODS层)
```

商务智能部 > 7.0 公司-基础数据应用组-工作内容 > image2018-4-20 14:17:54.png

针对不同的业务，BI架构组总结并实现了5种同步方式：

1. 全量同步，无分区；（适用于小表，参数表，全删全插）
2. 增量或者全量同步合并全量，按天分区；（适用于类似用户访问明细表，新增不修改，直接插入）
3. 增量同步合并全量，无分区（适用于类似用户表，直接插入）
4. 增量同步，有分区，抽取数据为目标表多个分区全量数据；（适用于类似支付订单表，按照分区字段覆盖分区插入）
5. 增量同步，有分区；（适用于类似订单表，按创建时间分区，最后修改时间抽取，按照非分区字段覆盖分区插入）

### 实时同步

有哪些技术方案？都是怎么实现的？

方案：

1. Spark Streaming 消费 mysql 的 binlog 日志
2. Spark Streaming 消费Kafka 日志

### 数据同步遇到的问题和技术方案

从数据本身、业务、数据开发人员等角度，总结下在数据同步上遇到过哪些问题？都是怎么解决的？

很长一段时间，是业务方的数据分析师或者业务产品从业务开发那里知道一些数据表，需要从中取数或者做报表，就会直接找BI这边的数据开发人员提数据同步的需求。时间一长，就是零零散散同步了很多表，却不知道这些表的业务背景是什么，为什么要同步这些表，同步之后又是怎么应用的，什么时候可以停掉。

## 离线数据开发

### 离线数据处理流程

离线数据都是怎么处理的？经过了那些流程？

大致流程如下：

> 【了解需求】→ 【模型设计】→ 【ETL开发】→ 【测试】→ 【发布上线】→ 【日常运维】→ 【任务下线】

如图所示

```mermaid
graph LR
A[了解需求] -->B(模型设计)
B --> C[ETL开发]
C --> D[测试]
D --> E[发布上线]
E  --> F[日常运维]
F --> G[任务下线]
```

> 按照维度建模理论，公司数仓包含两大类数据表：实时表和维表。

绝大部分实时表的表名为：fct_业务名，比如 fct_ordr，fct_ordr_pay等

绝大部分维表表的表名为：dim_枚举描述，比如 站点维表dim_site, 订单状态维表 dim_ordr_status, 城市维表 dim_city, 商品一级类目维表 dim_cate_level1等

 公司业务主题主要包括：订单、供应链、客服、流量。

## 实时数据开发

### 实时技术方案

方案：

1. spark streaming 消费 kafka 接收的流量消息，落地到HDFS.
2. hive load 命令将落地文件与对应的hive 外表关联

公司的折扣App的流量数据分散，需要启动多个Spark Streaming程序来独立消费每个Kafka Topic，然后将解析出来的数据存储到HDFS、HBASE或者Elastic Search，然后通过其他服务异步处理这些数据，进而得到目标数据。

数据采集端|源头|采集类型|目标|TOPIC|数据来源|实时log表
----- | ---- | ---- | ----- | ---- | ---| ----
PC\App内H5\m站|Sermon服务|页面|Kafka|abcplus_hash3|app端H5页面、PC端和m站的页面浏览数据|base.pc_extends_log_real
PC\App内H5\m站|Sermon服务|点击|Kafka|pc_events_hash3|app端H5页面、PC端和m站的点击数据|base.pc_event_log_real
App|Chant服务|启动|Kafka|mb_bootstrap_hash3|app端的启动数据|base.mb_bootstrap_real
App|Chant服务|轮播|Kafka|mb_lunbo_hash2|App中banner轮播|base.mb_lunbo_real
App|Chant服务|曝光|Kafka|mb_exposure_hash3|App端商品流、模块广告曝光数据|base.mb_exposure_real
App|Chant服务|页面|Kafka|mb_pageinfo_hash2|原生页面和RN页面的页面浏览数据|base.mb_pageinfo_real
App|Chant服务|点击|Kafka|mb_event_hash2|原生页面和RN页面的点击数据|base.mb_event_real
微信\微信小程序|WxApplet服务|交互|Kafka|wechat_interaction|
微信\微信小程序|WxApplet服务|启动|Kafka|wx_applet_bootstrap|小程序的启动数据|base.wx_bootstrap_real
微信\微信小程序|WxApplet服务|点击|Kafka|wx_applet_event|小程序中的用户点击数据|base.wx_event_real
微信\微信小程序|WxApplet服务|页面|Kafka|wx_applet_pageinfo|小程序中的用户页面浏览数据|base.wx_pageinfo_real

详细参见加架构组梳理的kafka topic集合：[5.埋点数据上报链路](http://wiki.abcplus.org/pages/viewpage.action?pageId=30012053)

## 数据模型

### 模型建设

#### 为什么要数据建模

> 数据模型就是数据组织和存储方法，它强调从业务、数据存取和使用的角度，合理有效的存储数据。
> “烂程序员关心的是代码，好程序员关心的是数据结构和它们之间的关系” – Linux 创始人 Linus Torvalds

好的数据模型关注以下几点：

- 性能：良好的数据模型能帮我们快速的查询所需要的数据，减少数据I/O的吞吐。
- 成本：良好的数据模型能极大的减少不必要的数据冗余，也能实现计算结果的复用，极大降低数据系统中的存储和计算成本。
- 效率：良好的数据模型能极大的改善用户使用数据的体验，提高使用数据的效率。
- 质量：良好的数据模型能够改善数据统计口径的不一致性，减少数据计算错误的可能性。

### 公司数据整合及管理

#### 术语 & 指标

指标管理需求wiki：http://wiki.abcplus.org/pages/viewpage.action?spaceKey=bi&title=OneDataV2.0

指标管理工具：http://onedata.abcplus.org

用OA账号登陆。

#### 公司数据仓库模型实施

### 纬度建模

#### 纬度设计

纬度是纬度建模的基础和灵魂。

纬度的作用是约束查询、分类汇总和排序。

目前数据仓库中的维表是：dim_开头的表

#### 事实设计

目前数据仓库中的事实表是 dw 库下以 fct_ 开头的表。这些表的字段中，有很多是_id 结尾的字段，这些字段属于纬度字段，并且绝大部分都有对应的维表。具体可以看下纬度设计中的表格。

## 数据管理

### 元数据

元数据管理工具采用开源工具以及公司内部开发的指标管理工具。

Apache Atlas 数据血缘管理
Atlas( https://hortonworks.com/apache/atlas) 是 HORTONWORKS 公司开源的一款数据血缘管理的工具。

关于HORTONWORKS: https://zh.hortonworks.com/ecosystems/

OneData指标管理工具
http://bi.abcplus.org

### Hive 任务优化

一个Hadoop集群可以提供的 map 和 reduce 的资源个数（也称为“插槽”）是固定的。某个大 job 可能会消耗完所有的插槽，从而导致其他 job 无法执行。通过设置属性 hive.exec.reducers.max 可以阻止某个查询消耗太多的 reducer 资源。

优化之前，需要对 MapReduce 的原理有个深刻的理解。参见：[7.0.1 基础数据应用岗位技术栈之MapReduce](http://wiki.abcplus.org/pages/viewpage.action?pageId=42731949)

#### Map倾斜

Map阶段是MR的起点，MR会预估本次计算的数据量，分配机器资源，然后尽可能将运算发送到数据存储的节点上，而不是移动数据。

因此，在启动阶段，如果小文件太多，就会导致 mapper 的数量巨大，调度和运行job的过程中产生太多的开销。

如果，文件多，并且文件大小不平衡，就会出现小文件的mapper计算完以后，还需要等待大文件的mapper执行完毕。

#### Join倾斜

当Hive执行Join时，需要选择哪个表被流式传输（stream），哪个表被缓存（cache）。 Hive将JOIN语句中的最后一个表用于流式传输，因此我们需要确保这个流表在两者之间是最大的。

在多表JOIN查询时，Hive假定查询的最后一张表是做大的表。在对每行记录进行连接操作时，它都会尝试将其他表缓存起来，然后扫描最后那个表进进行计算。因此用户需要保证连接查询中表的大小从左到右依次是增加的。

Hive提供了一个标记，来显示的告诉查询优化器哪张表是大表。

```sql
select
/*+STREAMTABLE(s)*/ s.page_id,p.page_name,count(1)
from dw.fct_page s
join dw.dim_page p on s.page_id = p.page_id
where s.date = '2018-04-20'
group by s.page_id,p.page_name;
```

如果所有表中有一张表的数量比较小，足够将所需的数据装入到内存中， 那么Hive可以执行一个map-side JOIN，将原本需要在reduce阶段执行的连接操作放到map端执行，可以减少reduce过程。

#### Reduce倾斜

Reduce 端负责对 Map 端梳理后的有序 key-value 键值对进行聚合，即进行Count、Sun、Avg 等聚合操作，得到最终的聚合结果。

Hive是根据输入的数据量大小来决定 reduce 的个数的。

### 存储优化

#### 数据存储格式

注意：此节主要是针对Hive。

我们在Hive的使用时，常用的存储格式有两种：TXT 和 ORC格式。

一般来讲，一些数据量比较小的表，比如商品类目表，数据量不过几千，直接采用TXT。但是，对于一些数据量很大的表，采用TXT格式机器浪费磁盘空间，因此采用的是ORC的格式，采用这种格式，磁盘的占用只有原来的十分之一。

#### 数据生命周期

### 数据质量

#### 数据质量衡量

![数据质量要求](http://olx1ji9hn.bkt.clouddn.com/image/%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F%E4%BF%9D%E9%9A%9C%E5%8E%9F%E5%88%992018.svg)

- 完整性：比如订单记录是否缺了，数据量波动异常等
- 准确性：数据中的信息是否正确，比如正整数的字段是否存在为0或者负数的情况等
- 一致性：同一份数据，在数据仓库中的类型、长度是否一致等
- 及时性：数据及时产出

#### 数据质量检查

按照数据质量的衡量原则，对关键作业设置质量检查。比如订单作业，当凌晨订单数据执行完以后，从数据量、销售额、订单状态字段的数据分布等方面与过去7天的平均值作对比。

详情请参阅：[8. 数据质量](http://wiki.abcplus.org/pages/viewpage.action?pageId=33342160) 一节。

## 数据培训

主要是对商务、运营、市场等业务方进行指标培训，以及引导业务合理提数据需求，培养业务方的数据意识。

对数据分析师进行指标、基础数据表培训、数据模型培训等，知道数据分析是如何快速的取数。

## 数据应用

在数据仓库初步成型以后，日常的基本工作是维护数据，保障数据质量，主要的工作的内容也会发生变化，需要将数据的价值体现出来。因此，此阶段的工作就是与业务结合，为业务提供数据支撑。

1. 营销服务

精准推送：分析购物车用户、分析用户领用券情况，，提供给业务用来定期精准推送；

摇钱树项目数据支撑，离线计算用户的完成的任务、积分排行等；

2. 精准化
配合精准化组，根据用户使用app情况，实现实时商品推荐；

配合精准化组，完成A/B实验的数据分析；

3. App页面流量分部
业务方需要了解app端各个页面、位置、素材的流量分部，基础数据应用团队负责数据支撑；

4. 报表平台：dataV
所有最新的dataV报表：[dataV-最新报表](http://wiki.abcplus.org/pages/viewpage.action?pageId=42737603)

基础数据应用团队为报表开发人员提供数据支撑和分析指导。

5. 运营分析工具 OLAP
- 类目运营分析
- 模块广告分析
- 成单路径分析

便于运营人员，比如类目运营查看各自负责的类目的运营情况。包页面、模块、位置、坑位的浏览、点击、销售等情况。
6. BOSS看板
提供宏观上的数据视图，领导层可以鸟瞰整个app端的7天、15天、30天、60天、90天的GMV趋势
每日新增用户（新增、注册、支付）、活跃用户分析（日活、周活、月活）、支付用户分析（新客、老客）。
全站用户访问、访问商详、加购、下单、支付的漏斗uv转化
分地域、分类目的销售额分布
7. 第三方数据分析工具-诸葛IO
[诸葛IO官方文档](http://docs.zhugeio.com/)

## 参考资料

- 阿里大数据实践: [阿里大数据实践](https://item.jd.com/28326834312.html)
- Hive 编程指南: [Hive 编程指南](https://item.jd.com/26793150880.html)
- 数据算法: [数据算法:Hadoop/Spark大数据处理技巧](https://item.jd.com/11993447.html)
- 绘图工具:
[mermaid-live-editor 在线编辑器](https://mermaidjs.github.io/mermaid-live-editor)
